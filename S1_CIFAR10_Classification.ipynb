{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S1-CIFAR10 Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salonigupta1/Image-Classifier/blob/main/S1_CIFAR10_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h7F2k529A7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfc277f-ae18-48f3-85e1-71479d78a4de"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "from os.path import isfile, isdir\n",
        "from tqdm import tqdm \n",
        "import tarfile\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import ops\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 960
        },
        "id": "NMLiDlkyPutW",
        "outputId": "f65957ce-ab03-47d8-b840-851d5d6eb44f"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "  Downloading tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6 MB 35 kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.17.3)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "  Downloading tensorboard-1.13.1-py3-none-any.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 39.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.37.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "  Downloading tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 45.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.12.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.39.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (4.6.4)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.6->tensorflow==1.13.1) (1.5.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.5.0)\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, keras-applications, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.6.0\n",
            "    Uninstalling tensorflow-estimator-2.6.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.6.0\n",
            "    Uninstalling tensorboard-2.6.0:\n",
            "      Successfully uninstalled tensorboard-2.6.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.6.0\n",
            "    Uninstalling tensorflow-2.6.0:\n",
            "      Successfully uninstalled tensorflow-2.6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.13.1 which is incompatible.\u001b[0m\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6r_8s_SPYO3",
        "outputId": "373bd67d-dede-43a8-9a3c-d87e122ae437"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ryd-b_59Nc9"
      },
      "source": [
        "cifar10_dataset_folder_path = 'cifar-10-batches-py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqsemgJn9XPK"
      },
      "source": [
        "class DownloadProgress(tqdm):\n",
        "    last_block = 0\n",
        "\n",
        "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
        "        self.total = total_size\n",
        "        self.update((block_num - self.last_block) * block_size)\n",
        "        self.last_block = block_num\n",
        "\n",
        "\"\"\" \n",
        "    check if the data (zip) file is already downloaded\n",
        "    if not, download it from \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" and save as cifar-10-python.tar.gz\n",
        "\"\"\"\n",
        "if not isfile('cifar-10-python.tar.gz'):\n",
        "    with DownloadProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
        "        urlretrieve(\n",
        "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
        "            'cifar-10-python.tar.gz',\n",
        "            pbar.hook)\n",
        "\n",
        "if not isdir(cifar10_dataset_folder_path):\n",
        "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
        "        tar.extractall()\n",
        "        tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N_AH58X9XoI"
      },
      "source": [
        "def load_label_names():\n",
        "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMAMm5MM9ghr"
      },
      "source": [
        "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
        "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
        "        # note the encoding type is 'latin1'\n",
        "        batch = pickle.load(file, encoding='latin1')\n",
        "        \n",
        "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "    labels = batch['labels']\n",
        "        \n",
        "    return features, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGzd1lnZvDvX"
      },
      "source": [
        "\n",
        "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
        "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
        "    \n",
        "    if not (0 <= sample_id < len(features)):\n",
        "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
        "        return None\n",
        "\n",
        "    print('\\nStats of batch #{}:'.format(batch_id))\n",
        "    print('# of Samples: {}\\n'.format(len(features)))\n",
        "    \n",
        "    label_names = load_label_names()\n",
        "    label_counts = dict(zip(*np.unique(labels, return_counts=True)))\n",
        "    for key, value in label_counts.items():\n",
        "        print('Label Counts of [{}]({}) : {}'.format(key, label_names[key].upper(), value))\n",
        "    \n",
        "    sample_image = features[sample_id]\n",
        "    sample_label = labels[sample_id]\n",
        "    \n",
        "    print('\\nExample of Image {}:'.format(sample_id))\n",
        "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
        "    print('Image - Shape: {}'.format(sample_image.shape))\n",
        "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
        "    \n",
        "    plt.imshow(sample_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlqGWfmpvH9o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "3985f8df-ab87-4316-88d4-542a0aaa41cc"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Explore the dataset\n",
        "batch_id = 3\n",
        "sample_id = 7000\n",
        "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stats of batch #3:\n",
            "# of Samples: 10000\n",
            "\n",
            "Label Counts of [0](AIRPLANE) : 994\n",
            "Label Counts of [1](AUTOMOBILE) : 1042\n",
            "Label Counts of [2](BIRD) : 965\n",
            "Label Counts of [3](CAT) : 997\n",
            "Label Counts of [4](DEER) : 990\n",
            "Label Counts of [5](DOG) : 1029\n",
            "Label Counts of [6](FROG) : 978\n",
            "Label Counts of [7](HORSE) : 1015\n",
            "Label Counts of [8](SHIP) : 961\n",
            "Label Counts of [9](TRUCK) : 1029\n",
            "\n",
            "Example of Image 7000:\n",
            "Image - Min Value: 24 Max Value: 252\n",
            "Image - Shape: (32, 32, 3)\n",
            "Label - Label Id: 0 Name: airplane\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "image/png": {
              "width": 251,
              "height": 248
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM2IYDbrvMEN"
      },
      "source": [
        "def normalize(x):\n",
        "    \"\"\"\n",
        "        argument\n",
        "            - x: input image data in numpy array [32, 32, 3]\n",
        "        return\n",
        "            - normalized x \n",
        "    \"\"\"\n",
        "    min_val = np.min(x)\n",
        "    max_val = np.max(x)\n",
        "    x = (x-min_val) / (max_val-min_val)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X1d3AZXvOUC"
      },
      "source": [
        "def one_hot_encode(x):\n",
        "    \"\"\"\n",
        "        argument\n",
        "            - x: a list of labels\n",
        "        return\n",
        "            - one hot encoding matrix (number of labels, number of class)\n",
        "    \"\"\"\n",
        "    encoded = np.zeros((len(x), 10))\n",
        "    \n",
        "    for idx, val in enumerate(x):\n",
        "        encoded[idx][val] = 1\n",
        "    \n",
        "    return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJi7C3N-vR6R"
      },
      "source": [
        "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
        "    features = normalize(features)\n",
        "    labels = one_hot_encode(labels)\n",
        "\n",
        "    pickle.dump((features, labels), open(filename, 'wb'))\n",
        "\n",
        "\n",
        "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
        "    n_batches = 5\n",
        "    valid_features = []\n",
        "    valid_labels = []\n",
        "\n",
        "    for batch_i in range(1, n_batches + 1):\n",
        "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
        "        \n",
        "        # find index to be the point as validation data in the whole dataset of the batch (10%)\n",
        "        index_of_validation = int(len(features) * 0.1)\n",
        "\n",
        "        # preprocess the 90% of the whole dataset of the batch\n",
        "        # - normalize the features\n",
        "        # - one_hot_encode the lables\n",
        "        # - save in a new file named, \"preprocess_batch_\" + batch_number\n",
        "        # - each file for each batch\n",
        "        _preprocess_and_save(normalize, one_hot_encode,\n",
        "                             features[:-index_of_validation], labels[:-index_of_validation], \n",
        "                             'preprocess_batch_' + str(batch_i) + '.p')\n",
        "\n",
        "        # unlike the training dataset, validation dataset will be added through all batch dataset\n",
        "        # - take 10% of the whold dataset of the batch\n",
        "        # - add them into a list of\n",
        "        #   - valid_features\n",
        "        #   - valid_labels\n",
        "        valid_features.extend(features[-index_of_validation:])\n",
        "        valid_labels.extend(labels[-index_of_validation:])\n",
        "\n",
        "    # preprocess the all stacked validation dataset\n",
        "    _preprocess_and_save(normalize, one_hot_encode,\n",
        "                         np.array(valid_features), np.array(valid_labels),\n",
        "                         'preprocess_validation.p')\n",
        "\n",
        "    # load the test dataset\n",
        "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding='latin1')\n",
        "\n",
        "    # preprocess the testing data\n",
        "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "    test_labels = batch['labels']\n",
        "\n",
        "    # Preprocess and Save all testing data\n",
        "    _preprocess_and_save(normalize, one_hot_encode,\n",
        "                         np.array(test_features), np.array(test_labels),\n",
        "                         'preprocess_training.p')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2a13dEXvUor"
      },
      "source": [
        "preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lq17nrq0vYjJ"
      },
      "source": [
        "import pickle\n",
        "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_HeSZRMvedp"
      },
      "source": [
        "# Remove previous weights, bias, inputs, etc..\n",
        "# tf.reset_default_graph()\n",
        "# tf.compat.v1.reset_default_graph()\n",
        "\n",
        "# import tensorflow.compat.v1 as tf_\n",
        "# tf_.disable_v2_behavior()\n",
        "\n",
        "# Inputs\n",
        "x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
        "y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
        "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3fTsgMEvhev"
      },
      "source": [
        "\n",
        "def conv_net(x, keep_prob):\n",
        "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
        "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
        "    conv3_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
        "    conv4_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))\n",
        "\n",
        "    # 1, 2\n",
        "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
        "    conv1 = tf.nn.relu(conv1)\n",
        "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
        "\n",
        "    # 3, 4\n",
        "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
        "    conv2 = tf.nn.relu(conv2)\n",
        "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
        "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
        "  \n",
        "    # 5, 6\n",
        "    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
        "    conv3 = tf.nn.relu(conv3)\n",
        "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
        "    conv3_bn = tf.layers.batch_normalization(conv3_pool)\n",
        "    \n",
        "    # 7, 8\n",
        "    conv4 = tf.nn.conv2d(conv3_bn, conv4_filter, strides=[1,1,1,1], padding='SAME')\n",
        "    conv4 = tf.nn.relu(conv4)\n",
        "    conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "    conv4_bn = tf.layers.batch_normalization(conv4_pool)\n",
        "    \n",
        "    # 9\n",
        "    flat = tf.contrib.layers.flatten(conv4_bn)  \n",
        "\n",
        "    # 10\n",
        "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
        "    full1 = tf.nn.dropout(full1, keep_prob)\n",
        "    full1 = tf.layers.batch_normalization(full1)\n",
        "    \n",
        "    # 11\n",
        "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
        "    full2 = tf.nn.dropout(full2, keep_prob)\n",
        "    full2 = tf.layers.batch_normalization(full2)\n",
        "    \n",
        "    # 12\n",
        "    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.relu)\n",
        "    full3 = tf.nn.dropout(full3, keep_prob)\n",
        "    full3 = tf.layers.batch_normalization(full3)    \n",
        "    \n",
        "    # 13\n",
        "    full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
        "    full4 = tf.nn.dropout(full4, keep_prob)\n",
        "    full4 = tf.layers.batch_normalization(full4)        \n",
        "    \n",
        "    # 14\n",
        "    out = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=10, activation_fn=None)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJcKHB8NvktS"
      },
      "source": [
        "epochs = 10\n",
        "batch_size = 64\n",
        "keep_probability = 0.7\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ahmde9qDvn4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe29dc3c-20fc-42e7-974d-1631501ce03b"
      },
      "source": [
        "logits = conv_net(x, keep_prob)\n",
        "model = tf.identity(logits, name='logits') # Name logits Tensor, so that can be loaded from disk after training\n",
        "\n",
        "# Loss and Optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# Accuracy\n",
        "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-16-f0f4d375bd45>:12: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-16-f0f4d375bd45>:37: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-18-bc29ffa8bb57>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zY-VvF8vqYy"
      },
      "source": [
        "\n",
        "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
        "    session.run(optimizer, \n",
        "                feed_dict={\n",
        "                    x: feature_batch,\n",
        "                    y: label_batch,\n",
        "                    keep_prob: keep_probability\n",
        "                })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGWNAka9vsdZ"
      },
      "source": [
        "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
        "    loss = sess.run(cost, \n",
        "                    feed_dict={\n",
        "                        x: feature_batch,\n",
        "                        y: label_batch,\n",
        "                        keep_prob: 1.\n",
        "                    })\n",
        "    valid_acc = sess.run(accuracy, \n",
        "                         feed_dict={\n",
        "                             x: valid_features,\n",
        "                             y: valid_labels,\n",
        "                             keep_prob: 1.\n",
        "                         })\n",
        "    \n",
        "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDoSUAQMvu7Z"
      },
      "source": [
        "def batch_features_labels(features, labels, batch_size):\n",
        "    \"\"\"\n",
        "    Split features and labels into batches\n",
        "    \"\"\"\n",
        "    for start in range(0, len(features), batch_size):\n",
        "        end = min(start + batch_size, len(features))\n",
        "        yield features[start:end], labels[start:end]\n",
        "\n",
        "def load_preprocess_training_batch(batch_id, batch_size):\n",
        "    \"\"\"\n",
        "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
        "    \"\"\"\n",
        "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
        "    features, labels = pickle.load(open(filename, mode='rb'))\n",
        "\n",
        "    # Return the training data in batches of size <batch_size> or less\n",
        "    return batch_features_labels(features, labels, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtXiiZK3vxu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccfe9710-5d22-45e0-c4c1-1c4acfd204e5"
      },
      "source": [
        "\n",
        "save_model_path = './image_classification'\n",
        "\n",
        "print('Training...')\n",
        "with tf.Session() as sess:\n",
        "    # Initializing the variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(epochs):\n",
        "        # Loop over all batches\n",
        "        n_batches = 5\n",
        "        for batch_i in range(1, n_batches + 1):\n",
        "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
        "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
        "                \n",
        "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
        "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
        "            \n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    save_path = saver.save(sess, save_model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training...\n",
            "Epoch  1, CIFAR-10 Batch 1:  Loss:     1.9893 Validation Accuracy: 0.294800\n",
            "Epoch  1, CIFAR-10 Batch 2:  Loss:     1.7428 Validation Accuracy: 0.280800\n",
            "Epoch  1, CIFAR-10 Batch 3:  Loss:     1.2443 Validation Accuracy: 0.430800\n",
            "Epoch  1, CIFAR-10 Batch 4:  Loss:     1.4384 Validation Accuracy: 0.466000\n",
            "Epoch  1, CIFAR-10 Batch 5:  Loss:     1.2335 Validation Accuracy: 0.526800\n",
            "Epoch  2, CIFAR-10 Batch 1:  Loss:     1.2107 Validation Accuracy: 0.568600\n",
            "Epoch  2, CIFAR-10 Batch 2:  Loss:     0.9287 Validation Accuracy: 0.593200\n",
            "Epoch  2, CIFAR-10 Batch 3:  Loss:     0.7823 Validation Accuracy: 0.630800\n",
            "Epoch  2, CIFAR-10 Batch 4:  Loss:     0.8542 Validation Accuracy: 0.596000\n",
            "Epoch  2, CIFAR-10 Batch 5:  Loss:     0.7368 Validation Accuracy: 0.671200\n",
            "Epoch  3, CIFAR-10 Batch 1:  Loss:     0.8832 Validation Accuracy: 0.680400\n",
            "Epoch  3, CIFAR-10 Batch 2:  Loss:     0.7025 Validation Accuracy: 0.679400\n",
            "Epoch  3, CIFAR-10 Batch 3:  Loss:     0.4890 Validation Accuracy: 0.661800\n",
            "Epoch  3, CIFAR-10 Batch 4:  Loss:     0.4699 Validation Accuracy: 0.714600\n",
            "Epoch  3, CIFAR-10 Batch 5:  Loss:     0.4762 Validation Accuracy: 0.705200\n",
            "Epoch  4, CIFAR-10 Batch 1:  Loss:     0.6922 Validation Accuracy: 0.711200\n",
            "Epoch  4, CIFAR-10 Batch 2:  Loss:     0.3412 Validation Accuracy: 0.713200\n",
            "Epoch  4, CIFAR-10 Batch 3:  Loss:     0.3089 Validation Accuracy: 0.691800\n",
            "Epoch  4, CIFAR-10 Batch 4:  Loss:     0.2987 Validation Accuracy: 0.712400\n",
            "Epoch  4, CIFAR-10 Batch 5:  Loss:     0.2897 Validation Accuracy: 0.728400\n",
            "Epoch  5, CIFAR-10 Batch 1:  Loss:     0.3822 Validation Accuracy: 0.727000\n",
            "Epoch  5, CIFAR-10 Batch 2:  Loss:     0.2963 Validation Accuracy: 0.725000\n",
            "Epoch  5, CIFAR-10 Batch 3:  Loss:     0.1663 Validation Accuracy: 0.718600\n",
            "Epoch  5, CIFAR-10 Batch 4:  Loss:     0.2236 Validation Accuracy: 0.732600\n",
            "Epoch  5, CIFAR-10 Batch 5:  Loss:     0.2049 Validation Accuracy: 0.715000\n",
            "Epoch  6, CIFAR-10 Batch 1:  Loss:     0.1504 Validation Accuracy: 0.731800\n",
            "Epoch  6, CIFAR-10 Batch 2:  Loss:     0.2029 Validation Accuracy: 0.738400\n",
            "Epoch  6, CIFAR-10 Batch 3:  Loss:     0.2179 Validation Accuracy: 0.709600\n",
            "Epoch  6, CIFAR-10 Batch 4:  Loss:     0.1735 Validation Accuracy: 0.732200\n",
            "Epoch  6, CIFAR-10 Batch 5:  Loss:     0.0653 Validation Accuracy: 0.749200\n",
            "Epoch  7, CIFAR-10 Batch 1:  Loss:     0.1025 Validation Accuracy: 0.722600\n",
            "Epoch  7, CIFAR-10 Batch 2:  Loss:     0.1678 Validation Accuracy: 0.740200\n",
            "Epoch  7, CIFAR-10 Batch 3:  Loss:     0.0556 Validation Accuracy: 0.754000\n",
            "Epoch  7, CIFAR-10 Batch 4:  Loss:     0.1260 Validation Accuracy: 0.745400\n",
            "Epoch  7, CIFAR-10 Batch 5:  Loss:     0.0813 Validation Accuracy: 0.735200\n",
            "Epoch  8, CIFAR-10 Batch 1:  Loss:     0.0443 Validation Accuracy: 0.737200\n",
            "Epoch  8, CIFAR-10 Batch 2:  Loss:     0.0545 Validation Accuracy: 0.728200\n",
            "Epoch  8, CIFAR-10 Batch 3:  Loss:     0.0542 Validation Accuracy: 0.724200\n",
            "Epoch  8, CIFAR-10 Batch 4:  Loss:     0.0392 Validation Accuracy: 0.730000\n",
            "Epoch  8, CIFAR-10 Batch 5:  Loss:     0.0331 Validation Accuracy: 0.739800\n",
            "Epoch  9, CIFAR-10 Batch 1:  Loss:     0.0313 Validation Accuracy: 0.745000\n",
            "Epoch  9, CIFAR-10 Batch 2:  Loss:     0.0409 Validation Accuracy: 0.715200\n",
            "Epoch  9, CIFAR-10 Batch 3:  Loss:     0.0351 Validation Accuracy: 0.726800\n",
            "Epoch  9, CIFAR-10 Batch 4:  Loss:     0.0244 Validation Accuracy: 0.743000\n",
            "Epoch  9, CIFAR-10 Batch 5:  Loss:     0.0392 Validation Accuracy: 0.737800\n",
            "Epoch 10, CIFAR-10 Batch 1:  Loss:     0.0418 Validation Accuracy: 0.742200\n",
            "Epoch 10, CIFAR-10 Batch 2:  Loss:     0.0336 Validation Accuracy: 0.738000\n",
            "Epoch 10, CIFAR-10 Batch 3:  Loss:     0.0109 Validation Accuracy: 0.757200\n",
            "Epoch 10, CIFAR-10 Batch 4:  Loss:     0.0350 Validation Accuracy: 0.728400\n",
            "Epoch 10, CIFAR-10 Batch 5:  Loss:     0.0207 Validation Accuracy: 0.751600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJV9CK7Ev1Uh"
      },
      "source": [
        "\n",
        "def batch_features_labels(features, labels, batch_size):\n",
        "    \"\"\"\n",
        "    Split features and labels into batches\n",
        "    \"\"\"\n",
        "    for start in range(0, len(features), batch_size):\n",
        "        end = min(start + batch_size, len(features))\n",
        "        yield features[start:end], labels[start:end]\n",
        "\n",
        "def display_image_predictions(features, labels, predictions, top_n_predictions):\n",
        "    n_classes = 10\n",
        "    label_names = load_label_names()\n",
        "    label_binarizer = LabelBinarizer()\n",
        "    label_binarizer.fit(range(n_classes))\n",
        "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
        "\n",
        "    fig, axies = plt.subplots(nrows=top_n_predictions, ncols=2, figsize=(20, 10))\n",
        "    fig.tight_layout()\n",
        "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
        "\n",
        "    n_predictions = 3\n",
        "    margin = 0.05\n",
        "    ind = np.arange(n_predictions)\n",
        "    width = (1. - 2. * margin) / n_predictions\n",
        "   \n",
        "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
        "        if (image_i < top_n_predictions):\n",
        "            pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
        "            correct_name = label_names[label_id]\n",
        "            \n",
        "            axies[image_i][0].imshow((feature*255).astype(np.int32, copy=False))\n",
        "            axies[image_i][0].set_title(correct_name)\n",
        "            axies[image_i][0].set_axis_off()\n",
        "\n",
        "            axies[image_i][1].barh(ind + margin, pred_values[:3], width)\n",
        "            axies[image_i][1].set_yticks(ind + margin)\n",
        "            axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
        "            axies[image_i][1].set_xticks([0, 0.5, 1.0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnTMWSFv4Gp"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "save_model_path = './image_classification'\n",
        "batch_size = 64\n",
        "n_samples = 10\n",
        "top_n_predictions = 5\n",
        "\n",
        "def test_model():\n",
        "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
        "    loaded_graph = tf.Graph()\n",
        "\n",
        "    with tf.Session(graph=loaded_graph) as sess:\n",
        "        # Load model\n",
        "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
        "        loader.restore(sess, save_model_path)\n",
        "\n",
        "        # Get Tensors from loaded model\n",
        "        loaded_x = loaded_graph.get_tensor_by_name('input_x:0')\n",
        "        loaded_y = loaded_graph.get_tensor_by_name('output_y:0')\n",
        "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
        "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
        "        \n",
        "        # Get accuracy in batches for memory limitations\n",
        "        test_batch_acc_total = 0\n",
        "        test_batch_count = 0\n",
        "        \n",
        "        for train_feature_batch, train_label_batch in batch_features_labels(test_features, test_labels, batch_size):\n",
        "            test_batch_acc_total += sess.run(\n",
        "                loaded_acc,\n",
        "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
        "            test_batch_count += 1\n",
        "\n",
        "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
        "\n",
        "        # Print Random Samples\n",
        "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
        "        random_test_predictions = sess.run(\n",
        "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
        "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
        "        display_image_predictions(random_test_features, random_test_labels, random_test_predictions, top_n_predictions)\n",
        "\n",
        "\n",
        "test_model()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}